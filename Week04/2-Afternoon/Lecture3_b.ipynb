{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CSS 201.5 - CSS Bootcamp\n",
    "\n",
    "## Week 04 - Lecture 3 (afternoon)\n",
    "\n",
    "### Umberto Mignozzetti (UCSD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Counting methods\n",
    "\n",
    "**Permutations:** \n",
    "\n",
    "1. Set with $n$ elements.\n",
    "2. Consider an experiment that selects $k$ of the elements one at a time *without replacement*.\n",
    "3. Let each outcome consist of the $k$ elements in the order selected.\n",
    "4. Each such outcome is called a permutation of $n$ elements taken $k$ at a time.\n",
    "\n",
    "We denote the number of distinct such permutations by the symbol $P_{n, k} = n \\times (n-1) \\cdots (n-k-1)$.\n",
    "\n",
    "Using factorials:\n",
    "\n",
    "$$P_{n, k} = \\dfrac{n!}{(n-k)!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Counting methods\n",
    "\n",
    "**Sampling with Replacement**: Consider a box that contains $n$ balls numbered $1$ to $n$.\n",
    "\n",
    "First, one ball is selected at random from the box and its number is noted. \n",
    "\n",
    "This ball is then put back in the box and another ball is selected (it is possible that the same ball will be selected again). \n",
    "\n",
    "As many balls as desired can be selected in this way.\n",
    "\n",
    "This process is called **sampling with replacement**. \n",
    "\n",
    "It is assumed that each of the $n$ balls is equally likely to be selected at each stage and that all selections are made independently of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Counting methods\n",
    "\n",
    "**Sampling with Replacement**:\n",
    "\n",
    "Suppose that $k$ selections were made. \n",
    "\n",
    "1. What is the total number of possible outcomes?\n",
    "1. What is the chance of these outcomes?\n",
    "\n",
    "- What is the chance of, after drawing $k$ balls, they have all different numbers?\n",
    "\n",
    "$$p = \\dfrac{P_{n, k}}{n^k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Counting methods\n",
    "\n",
    "**Check-in**: I know we talked about this, but it is a good way to recap:\n",
    "\n",
    ">What is the chance that in a group, two people would have the same birthday?\n",
    "\n",
    "1. Easier Q: What is the chance that all people have different birthdays?\n",
    "\n",
    "1. Then what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combinatorial Methods\n",
    "\n",
    "Still in counting stuff, suppose we need to count how many subsets of $\\{a, b, c, d \\}$ with two elements we can list.\n",
    "\n",
    "What is the problem? \n",
    "\n",
    ">$\\{a, b\\}$ and $\\{b, a\\}$ are the same!\n",
    "\n",
    "Can we use permutations? Try it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combinatorial Methods\n",
    "\n",
    "Permutations are constructed in two steps:\n",
    "\n",
    "1. A combination of $k$ elements is chosen out of $n$\n",
    "2. Those $k$ elements are ***arranged in a specific order***.\n",
    "\n",
    "Say there are $C_{n, k}$ ways (don't bother with what $C$ is right now, think it is just a number) to choose the $k$ elements out of $n$\n",
    "\n",
    "But for each such choice there are $k!$ ways to arrange those k elements in different orders.\n",
    "\n",
    "Using the multiplication rule, the number of permutations of $n$ elements taken $k$ at a time is $P_{n, k}$.\n",
    "\n",
    "Hence:\n",
    "\n",
    "$$C_{n, k} = \\dfrac{P_{n, k}}{k!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combinatorial Methods\n",
    "\n",
    "**Combinations**: \n",
    "\n",
    "1. Set with $n$ elements.\n",
    "2. And we want to extract $k$ **distinct subsets**!\n",
    "3. Each subset $k$ chosen from this set is called a *combination of $n$ elements taken $k$ at a time.*\n",
    "4. We denote it by $C_{n, k}$.\n",
    "\n",
    "$$C_{n, k} = {n \\choose k} = \\dfrac{P_{n,k}}{k!} = \\dfrac{n!}{k!(n-k)!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combinatorial Methods (check-in)\n",
    "\n",
    "*Selecting a Committee.* \n",
    "\n",
    "Suppose a committee composed of eight representatives is to be selected from a group of 20 representatives. \n",
    "\n",
    "What is the number of different groups of representatives that might be on the committee?\n",
    "\n",
    "$$C_{n, k} = {n \\choose k} = \\dfrac{P_{n,k}}{k!} = \\dfrac{n!}{k!(n-k)!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combinatorial Methods\n",
    "\n",
    "**Combinations -- Rules**:\n",
    "\n",
    "1. $C_{n, k} = {n \\choose k} = \\dfrac{P_{n,k}}{k!} = \\dfrac{n!}{k!(n-k)!}$\n",
    "1. ${n \\choose 0} = {n \\choose n} = 1$\n",
    "1. ${n \\choose k} = {n \\choose n-k}$\n",
    "1. $n + {n \\choose 2} = {n+1 \\choose 2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combinatorial Methods (check-in)\n",
    "\n",
    "**Combinations**:\n",
    "\n",
    "*Tossing a Coin.* Suppose that a fair coin is to be tossed 10 times, and suppose we want to find:\n",
    "\n",
    "1. The probability $p$ of obtaining exactly three heads \n",
    "1. The probability $p$ of obtaining three or fewer heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combinatorial Methods (check-in)\n",
    "\n",
    "Prove that:\n",
    "\n",
    "$${n \\choose k} + {n \\choose k-1} = {n+1 \\choose k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combinatorial Methods\n",
    "\n",
    "**Combinations**: *Sampling without Replacement.*\n",
    "\n",
    "Suppose that a class contains 15 boys and 30 girls, and that 10 students are to be selected at random for a special assignment. What is the probability $p$ that exactly three boys will be selected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combinatorial Methods (check-in)\n",
    "\n",
    "1. Two pollsters will canvas a neighborhood with 20 houses. Each pollster will visit 10 of the houses. How many different assignments of pollsters to houses are possible?\n",
    "\n",
    "1. A box contains 24 light bulbs, of which four are defective. If a person selects four bulbs from the box at random, without replacement, what is the probability that all four bulbs will be defective?\n",
    "\n",
    "1. The United States Senate contains two senators from each of the 50 states.\n",
    "    1. If a committee of eight senators is selected at random, what is the probability that it will contain at least one of the two senators from a certain specified state?\n",
    "    2. What is the probability that a group of 50 senators selected at random will contain one senator from each state?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combinatorial Methods\n",
    "\n",
    "Another common problem is to, instead of draw two distinct groups, draw many distict groups.\n",
    "\n",
    "Example (*Choosing Committees*): Suppose that 20 members of an organization are to be divided into three committees A, B, and C in such a way that each of the committees A and B is to have eight members and committee C is to have four members. We shall determine the number of different ways in which members can be assigned to these committees. Notice that each of the 20 members gets assigned to one and only one committee. \n",
    "\n",
    "Method:\n",
    "1. Choose the first\n",
    "1. Split the remaining in the second\n",
    "1. Done with third\n",
    "\n",
    "Let's see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combinatorial Methods\n",
    "\n",
    "In general, suppose that $n$ distinct elements are to be divided into $k$ different groups ($k \\geq 2$).\n",
    "\n",
    "We want to divide in such a way that, for $j = 1, \\cdots, k$, the j-th group contains exactly $n_j$ elements, where $n_1 + n_2 + \\cdots + n_k = n$. \n",
    "\n",
    "In how many different ways the $n$ elements can be divided into the $k$ groups?\n",
    "\n",
    "$${n \\choose n_1}{n-n_1 \\choose n_2}{n-n_1-n_2 \\choose n_3}\\cdots{n-n_1-n_2-\\cdots-n_{k-2} \\choose n_{k-1}} = \\dfrac{n!}{n_1!n_2! \\cdots n_k! }$$\n",
    "\n",
    "**Definition -- Multinomial Coefficients**: The number $\\dfrac{n!}{n_1!n_2! \\cdots n_k! }$ denoted by ${n \\choose n_1, n_2, \\cdots, n_k}$ is called multinomial coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combinatorial Methods\n",
    "\n",
    "**Properties:** \n",
    "\n",
    "1. $(x_1 + \\cdots + x_n)^n = \\sum {n \\choose n_1, n_2, \\cdots, n_k} x_1^{n_1}x_2^{n_2}\\cdots x_k^{n_k}$\n",
    "1. ${n \\choose k, n - k} = {n \\choose k}$\n",
    "1. Just as binomials determine arrangement of two different types, multinomial determine arranges of $k$ different types.\n",
    "\n",
    "Example (*Rolling Dice*): Suppose that 12 dice are to be rolled. What is the probability $p$ that each of the six different numbers will appear twice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fundamentals of Probability Theory\n",
    "\n",
    "**Check-in:** \n",
    "\n",
    "1. Three pollsters will canvas a neighborhood with 21 houses. Each pollster will visit seven of the houses. How many different assignments of pollsters to houses are possible?\n",
    "\n",
    "1. Suppose that 18 red beads, 12 yellow beads, eight blue beads, and 12 black beads are to be strung in a row. How many different arrangements of the colors can be formed?\n",
    "\n",
    "1. If the letters s, s, s, t , t , t , i, i, a, c are arranged in a random order, what is the probability that they will spell the word \"statistics\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Probability\n",
    "\n",
    "We rarely talk about one probability only. In science, we are interested in how thing relate to each other. \n",
    "\n",
    "A good start is to see *events relate to each other.*\n",
    "\n",
    "**Definition -- Conditional Probability**: For $A, B \\in \\mathcal{S}$ (with $P(B)>0$), the **joint probability of A and B** is \n",
    "\n",
    "$$ P(A|B) \\ = \\dfrac{P(A \\cap B)}{P(B)} $$\n",
    "\n",
    "![img](https://github.com/umbertomig/CSSBootCamp/blob/main/img/cond1.png?raw=true)\n",
    "\n",
    "We read: The probability of the event $A$ happening, given that we know that $B$ happened.\n",
    "\n",
    "Or, for short:\n",
    "\n",
    ">Probability of $A$ given $B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fundamentals of Probability Theory\n",
    "\n",
    "### Conditional Probability\n",
    "\n",
    "Example (*A Clinical Trial*): Prien et al. (1984) studied three treatments for depression: imipramine, lithium carbonate, a combination, and a placebo.\n",
    "\n",
    "In this example, we shall consider 150 patients who entered the study after an episode of depression that was classified as “unipolar” (meaning that there was no manic disorder). \n",
    "\n",
    "They were divided into the four groups (three treatments plus placebo) and followed to see how many had recurrences of depression.\n",
    "\n",
    "![img](https://github.com/umbertomig/CSSBootCamp/blob/main/img/cond2.png?raw=true)\n",
    "\n",
    "If a patient were selected at random from this study and it were found that the patient received the placebo treatment, what is the conditional probability that the patient had a relapse?\n",
    "\n",
    "**Theorem: Multiplicative Law**: For $A, B \\in \\mathcal{S}$, with $P(B) > 0$, then $P(A | B)P(B) = P(A \\cap B)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fundamentals of Probability Theory\n",
    "\n",
    "**Theorem -- Multiplicative Law**: Let $A$ and $B$ events. \n",
    "\n",
    "1. If with $P(B) > 0$, then $P(A | B)P(B) = P(A \\cap B)$.\n",
    "\n",
    "2. If with $P(A) > 0$, then $P(B | A)P(A) = P(A \\cap B)$.\n",
    "\n",
    "Example (*Selecting Two Balls*): Suppose that two balls are to be selected at random, without replacement, from a box containing $r$ red balls and $b$ blue balls. What is the probability p that the first ball is red and the second ball is blue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fundamentals of Probability Theory\n",
    "\n",
    "**Definition -- Partition**: If $\\{B_{1},B_{2},\\cdots \\}$, with events $B_{i}$, are non-empty and pairwise disjoint sets such that $S = \\cup_i B_{i}$, the set $\\{B_{1},B_{2},\\cdots\\}$ is called a *partition* of $S$.\n",
    "\n",
    "**Theorem -- Law of Total Probability**: Let $\\{ B_{1}, B_{2}, \\cdots \\}$ is a partition of $S$, and $A$ is an event, then\n",
    "\n",
    "$$ P(B) \\ = \\ \\sum_{i} P(B \\cap A_{i}) $$\n",
    "\n",
    "And if, $\\forall i$,  $P(A_{i}) > 0$, then\n",
    "\n",
    "$$ P(B) \\ = \\ \\sum_{i} P( B | A_{i}) P(A_{i}) $$\n",
    "\n",
    "![img](https://github.com/umbertomig/CSSBootCamp/blob/main/img/cond3.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fundamentals of Probability Theory\n",
    "\n",
    "Example (*Achieving a High Score.*): Suppose that a person plays a game in which his score must be one of the 50 numbers $1, 2, \\cdots, 50$ and that each of these 50 numbers is equally likely to be his score.\n",
    "\n",
    "1. The first time she plays the game, her score is X. \n",
    "\n",
    "1. She then continues to play the game until she obtains another score $Y$ such that $Y geq X$.\n",
    "\n",
    "We will assume that, conditional on previous plays, the 50 scores remain equally likely on all subsequent plays. \n",
    "\n",
    "What is the probability of the event $A$ that $Y = 50$?\n",
    "\n",
    "1. For each $i = 1, \\cdots, 50$, let $B_i$ be the event that $X = i$. \n",
    "\n",
    "1. Conditional on $B_i$, the value of Y is equally likely to be any one of the numbers $i, i + 1, \\cdots, 50$. Since each of these $(51−i)$ possible values for $Y$ is equally likely, it follows that\n",
    "\n",
    "$$P(A|B_i) = P(Y=50|B_i) = \\dfrac{1}{51-i}$$\n",
    "\n",
    "How much is $P(A)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fundamentals of Probability Theory\n",
    "\n",
    "**Check-in:**\n",
    "\n",
    "1. If A and B are disjoint events and $P(B) > 0$, what is the value of $P(A|B)$?\n",
    "\n",
    "1. Each time a shopper purchases a tube of toothpaste, she chooses either brand A or brand B. Suppose that for each purchase after the first, the probability is 1/3 that she will choose the same brand that she chose on his preceding purchase and the probability is 2/3 that she will switch brands. If she is equally likely to choose either brand A or brand B on her first purchase, what is the probability that both her first and second purchases will be brand A and both her third and fourth purchases will be brand B?\n",
    "\n",
    "1. A box contains three coins with a head on each side, four coins with a tail on each side, and two fair coins. If one of these nine coins is selected at random and tossed once, what is the probability that a head will be obtained?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Independence\n",
    "\n",
    "**Definition -- Independence of Events**: Events $A, B$ are *independent* if $P(A \\cap B) = P(A)P(B)$\n",
    "\n",
    "**Theorem -- Conditional Probability and Independence**: For $A, B$, with $P(B) > 0$, $A$ and $B$ are independent if, and only if, $P(A|B) = P(A)$.\n",
    "\n",
    "In words, the events are independent if, upon learning about $B$, we do not change our views about $A$.\n",
    "\n",
    "**Theorem -- Independence of Complements**: If two events $A$ and $B$ are independent, then the events $A$ and $B^c$ are also independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Independence\n",
    "\n",
    "Example (*Inspecting Items*). Suppose that a machine produces a defective item with probability p ($0 \\leq p \\leq 1$) and produces a nondefective item with probability $1−p$. Suppose further that six items produced by the machine are selected at random and inspected, and that the results (defective or nondefective) for these six items are independent.\n",
    "\n",
    "1. What are the chances that exactly two of the six items are defective.\n",
    "\n",
    "1. What is the chance that at least one item will be defective?\n",
    "\n",
    "1. What is the probability $p_n$ that exactly $n$ items ($n \\geq 5$) must be selected to obtain the five defectives? (think about $n-1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Independence\n",
    "\n",
    "Example (*People v. Collins.*):\n",
    "\n",
    "- Finkelstein and Levin (1990) describe a criminal case whose verdict was overturned by the Supreme Court of California in part due to a probability calculation involving both conditional probability and independence. \n",
    "\n",
    "- The case, People v. Collins, 68 Cal. 2d 319, 438 P.2d 33 (1968), involved a purse snatching in which witnesses claimed to see a young woman with blond hair in a ponytail fleeing from the scene in a yellow car driven by a black man with a beard. \n",
    "\n",
    "- A couple meeting the description was arrested a few days after the crime, but no physical evidence was found.\n",
    "\n",
    "- A mathematician calculated the probability that a randomly selected couple would possess the described characteristics as about $8.3 \\times 10^{−8}$, or 1 in 12 million. \n",
    "\n",
    "- Faced with such overwhelming odds and no physical evidence, the jury decided that the defendants must have been the only such couple and convicted them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Independence\n",
    "\n",
    "Example (*People v. Collins.*):\n",
    "\n",
    "- The Supreme Court thought that a more useful probability should have been calculated. \n",
    "\n",
    "- Based on the testimony of the witnesses, there was a couple that met the above description. Given that there was already one couple who met the description, ***what is the conditional probability that there was also a second couple such as the defendants?***\n",
    "\n",
    "- Let $p$ be the probability that a randomly selected couple from a population of $n$ couples has certain characteristics. \n",
    "\n",
    "- Let $A$ be the event that at least one couple in the population has the characteristics, and let $B$ be the event that at least two couples have the characteristics.\n",
    "\n",
    "- What is $P(B|A)$? Since $B \\subset A$:\n",
    "\n",
    "$$P(B|A) = \\dfrac{P(B \\cup A)}{P(A)} = \\dfrac{P(B)}{P(A)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Independence\n",
    "\n",
    "Example (*People v. Collins.*):\n",
    "\n",
    "- To compute $P(A)$ and $P(B)$, suppose we number couples in the population $1, 2, \\cdots, n$. \n",
    "\n",
    "- Let $A_i$ the event that couple $i$ has the characteristic. Then, (A) ***the event that at least one couple has the characteristic*** is equal to:\n",
    "\n",
    "$$A = (A_1^C \\cap A_2^C \\cap \\cdots \\cap A_n^C)^C$$\n",
    "\n",
    "- Now, let us consider (C) ***the chance that exactly one couple has this characteristic***:\n",
    "\n",
    "$$C = (A_1 \\cap A_2^C \\cap \\cdots \\cap A_n^C) \\cup (A_1^C \\cap A_2 \\cap \\cdots \\cap A_n^C) \\cup \\cdots \\cup (A_1^C \\cap A_2^C \\cap \\cdots \\cap A_n)$$\n",
    "\n",
    "- Now $B$, which is (B) ***the event that at least two couples have the characteristics***, is equal to:\n",
    "\n",
    "$$B = A \\cap C^C$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Independence\n",
    "\n",
    "Example (*People v. Collins.*):\n",
    "\n",
    "- Assuming the $n$ couples are mutually independent:\n",
    "\n",
    "    - $P(A^C) = (1-p)^n$ and $P(A) = 1 - (1-p)^n$\n",
    "    - $P(C) = np(1-p)^{n-1}$\n",
    "    - The chance that at least two couples have the same characteristics is equal to the chance that at least one minus exactly one. $P(B) = P(A) - P(C) = 1-(1-p)^n - np(1-p)^{n-1}$\n",
    "\n",
    "$$P(B|A) = \\dfrac{1-(1-p)^n - np(1-p)^{n-1}}{1 - (1-p)^n}$$\n",
    "\n",
    "- And if $n = 8,000,000$ and $p = 8.3 \\times 10^{-8}$, $P(B|A) = 0.2966$. \n",
    "\n",
    "- Thus, the Supreme Court ruled that there was enough grounds for reasonable doubt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Independence\n",
    "\n",
    "**Check-in**:\n",
    "\n",
    "1. Suppose that two machines 1 and 2 in a factory are operated independently of each other. Let $A$ be the event that machine 1 will become inoperative during a given 8-hour period, let $B$ be the event that machine 2 will become inoperative during the same period, and suppose that $P(A) = 1/3$ and $P(B) = 1/4$. What is the chance that at least one machine will become inoperative during a given period?\n",
    "\n",
    "1. Use the fact that $P(A \\cap B^C) = P(A) - P(A \\cap B)$ to prove that if two events $A$ and $B$ are independent, then the events $A$ and $B^C$ are also independent.\n",
    "\n",
    "1. Suppose that a fair coin is tossed until a head appears for the first time, and assume that the outcomes of the tosses are independent. What is the probability $p_n$ that exactly $n$ tosses will be required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fundamentals of Probability Theory\n",
    "\n",
    "One of the most important consequences of this reasoning is the *Bayes theorem*.\n",
    "\n",
    "1. It provides a foundation for us to update the predictive probability of events\n",
    "1. Example: Suppose that we want to predict an event $A$. If we observe an event $B$, we can use the information of $B$ to improve our understanding about the chance of $A$ occuring.\n",
    "1. Concrete example: Suppose you want to go to the beach (yes, please, desperately...). Then, if you see a cloud in the sky, you can update your accessment of the chance that it is going to rain.\n",
    "\n",
    "**Theorem -- Bayes Rule**: For $A, B$ events, with $P(A) > 0$ and $P(B) > 0$, \n",
    "\n",
    "$$ P(A | B) \\ = \\ \\dfrac{P(B|A)P(A)}{P(B)} $$\n",
    "\n",
    "**Theorem -- Bayes Theorem (general)**: Let the events $B_1, B_2, \\cdots, B_k$ form a partition of $S$ such that $P(B_i) > 0$ for all $i \\in \\{1, \\cdots, k\\}$. Then:\n",
    "\n",
    "$$P(B_i|A) \\ = \\ \\dfrac{P(A|B_i)P(B_i)}{\\sum_{j=1}^kP(B_j)P(A|B_j)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fundamentals of Probability Theory\n",
    "\n",
    "**Check-in**: \n",
    "\n",
    "1. Suppose that you are walking down the street and notice that the Department of Public Health is giving a free medical test for a certain disease. The test is 90 percent reliable in the following sense: If a person has the disease, there is a probability of 0.9 that the test will give a positive response; whereas, if a person does not have the disease, there is a probability of only 0.1 that the test will give a positive response. $\\newline$ Data indicate that your chances of having the disease are only 1 in 10,000. However, since the test costs you nothing, and is fast and harmless, you decide to stop and take the test. A few days later you learn that you had a positive response to the test. Now, what is the probability that you have the disease? $\\newline\\newline$\n",
    "\n",
    "1. In a certain city, 30 percent of the people are Conservatives, 50 percent are Liberals, and 20 percent are Independents. Records show that in a particular election, 65 percent of the Conservatives voted, 82 percent of the Liberals voted, and 50 percent of the Independents voted. If a person in the city is selected at random and it is learned that she did not vote in the last election, what is the probability that she is a Liberal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\newcommand\\pN{N}$\n",
    "$\\newcommand\\pE{\\mathop{{}\\mathbb{E}}}$\n",
    "$\\newcommand\\pV{\\mathop{{}\\mathbb{V}}}$\n",
    "$\\newcommand\\pP{\\mathop{{}\\mathbb{P}}}$\n",
    "$\\newcommand{\\ind}{\\perp\\!\\!\\!\\!\\perp}$\n",
    "$\\newcommand{\\bX}{\\mathbf{X}}$\n",
    "\n",
    "\n",
    "# Random Variables\n",
    "\n",
    "$\\newcommand{\\by}{\\mathbf{y}}$\n",
    "$\\newcommand{\\be}{\\mathbf{y}}$\n",
    "$\\newcommand{\\bbs}{\\mathbf{b}}$\n",
    "$\\newcommand{\\bbeta}{\\mathbf{\\beta}}$\n",
    "$\\newcommand{\\hbbeta}{\\widehat{\\mathbf{\\beta}}}$\n",
    "$\\newcommand{\\bhb}{\\widehat{\\mathbf{b}}}$\n",
    "$\\newcommand{\\bvx}{\\mathbf{x}}$ \n",
    "$\\newcommand{\\Ex}{\\mathbb{E}}$\n",
    "$\\newcommand{\\Vax}{\\mathbb{V}}$\n",
    "$\\newcommand{\\real}{\\mathbb{R}}$\n",
    "$\\newcommand{\\realp}{\\mathbb{R}^{+}}$\n",
    "$\\newcommand{\\cov}{\\text{Cov}}$\n",
    "$\\newcommand{\\convp}{\\overset{p}{\\longrightarrow}}$\n",
    "$\\newcommand{\\convd}{\\overset{d}{\\longrightarrow}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Variable\n",
    "\n",
    "**Definition: Random Variable**: Let $S$ be the sample space for an experiment. A real-valued function that is defined on $S$ is called a random variable.\n",
    "\n",
    "A random variable maps each state of the world to a real number.\n",
    "    \n",
    "1. Consider an experiment where a fair coin is tossed 10 times. We can define a random variable $X$ that counts the number of heads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Variable\n",
    "\n",
    "When a probability measure has been defined on the sample space, we can determine the probabilities associated with each possible value of the random variable.\n",
    "\n",
    "**Definition -- Distribution of a random variable**: Let $X$ be a random variable. The **distribution** of $X$ is the collection of all probabilities of the form $P(X \\in C)$ for all sets $C$ of real numbers such that $\\{X \\in C\\}$ is an event.\n",
    "\n",
    "Example: Let a fair coin be tossed 10 times,\n",
    "and let $X$ be the number of heads that are obtained. In this experiment, the possible values of $X$ are $0, 1, 2, \\cdots, 10$. \n",
    "\n",
    "For each $x$, $P(X = x)$ is the sum of the probabilities\n",
    "of all of the outcomes in the event $\\{X = x\\}$. \n",
    "\n",
    "Because the coin is fair, each outcome has the same probability $\\big(\\frac{1}{2}\\big)^{10}$, and we need only count how many outcomes $s$ have $X(s) = x$.\n",
    "\n",
    "We know that $X(s) = x$ if and only if exactly $x$ of the 10 tosses are $H$.\n",
    "\n",
    "How much is this probabily?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Variable\n",
    "\n",
    "**Definition - Discrete Random Variable**: A *random variable* $X$ is discrete if takes at most $k$ values or an infinite sequence of countable values.\n",
    "\n",
    "Example: In the 10 times coin toss example, each outcome has chance $\\dfrac{1}{2^{10}}$. But there are ${10 \\choose x}$ number of events with $x$ heads. Therefore:\n",
    "\n",
    "$$P(X = x) = {10 \\choose x}\\dfrac{1}{2^{10}}$$\n",
    "\n",
    "**Definition - Probability Mass Function (PMF) and Support**:\n",
    "\n",
    "- Probability mass function: $f(x) = P(X = x)$\n",
    "\n",
    "- Support of a probability function: The closure of the set $\\{x: f(x) > 0\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Variable\n",
    "\n",
    "**Theorem**: If $X$ has a discrete distribution, the the probability of each subset $C$ in the real line is:\n",
    "\n",
    "$$P(X \\in C) = \\sum_{x_i \\in C} f(x_i)$$\n",
    "\n",
    "**Theorem**: Let $X$ be a discrete random variable with p.f. (probability function) $f$. \n",
    "\n",
    "1. If $x$ is not one of the possible values of $X$, then $f(x) = 0$.\n",
    "1. If the sequence $x_1, x_2, \\cdots$ includes all the possible values of $X$, then $\\sum_{i=1}^{\\infty}f(x_i) = 1$.\n",
    "\n",
    "**Definition - Bernoulli Distribution**: A random variable $Z$ that takes only two values $0$ and $1$ with $P(Z = 1) = p$ has the ***Bernoulli distribution*** with parameter p. We also say that $Z$ is a Bernoulli random variable with parameter $p$ and denote $Z \\sim Bernoulli(p)$\n",
    "\n",
    "The distribution of the variable is:\n",
    "\n",
    "$$\n",
    "f(x) =\n",
    "  \\begin{cases}\n",
    "    1-p & \\text{for $x = 0$} \\\\\n",
    "    p & \\text{for $x = 1$} \\\\\n",
    "    0 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Variable\n",
    "\n",
    "**Definition - Uniform Distribution**: Let $a \\leq b$ integers. Suppose that the value of a random variable $X$ is equally likely to be each of the integers $a, \\cdots, b$. Then we say that $X$ has the uniform distribution on the integers $a, \\cdots, b$.\n",
    "\n",
    "The distribution of the variable is:\n",
    "\n",
    "$$\n",
    "f(x) =\n",
    "  \\begin{cases}\n",
    "    \\dfrac{1}{b - a + 1} & \\text{for $x = a, \\cdots, b$} \\\\\n",
    "    0 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Variable\n",
    "\n",
    "Suppose that we want to model the distribution of votes for a given politician. If each person has chance $p$, the number of voters is $n$. Then, the chance that the politician gets $x$ votes is:\n",
    "\n",
    "$$P(X = x) = {n \\choose x} p^x(1-p)^{n-x}$$\n",
    "\n",
    "**Definition - Binomial Distribution**: The discrete distribution represented by the probability function\n",
    "\n",
    "$$\n",
    "f(x) =\n",
    "  \\begin{cases}\n",
    "    {n \\choose x} p^x(1-p)^{n-x} & \\text{for $x = 0, 1, \\cdots, n$} \\\\\n",
    "    0 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "is called the binomial distribution with parameters $n$ and $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Variable\n",
    "\n",
    "Examples:\n",
    "\n",
    "1. Suppose that two balanced dice are rolled, and let $X$ denote the absolute value of the difference between the two numbers that appear. Determine and sketch the p.f. of $X$.\n",
    "\n",
    "1. Suppose that a random variable X has a discrete distribution with the following probability function: $$\n",
    "f(x) =\n",
    "  \\begin{cases}\n",
    "    cx & \\text{for $x = 1, \\cdots, 5$} \\\\\n",
    "    0 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "$$ Determine the value of the constant $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Variable\n",
    "\n",
    "**Check-in**\n",
    "\n",
    "1. Suppose that a random variable X has the uniform distribution on the integers $10, \\cdots, 20$. Find the probability that X is even.\n",
    "\n",
    "1. If 10 percent of the balls in a certain box are red, and if 20 balls are selected from the box at random, with replacement, what is the probability that more than three red balls will be obtained?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Continuous Distribution\n",
    "\n",
    "**Definition -- Continuous Distribution:** We say that $X$ is a continuous random variable if there exists a\n",
    "nonnegative function $f$, defined on the real line, such that for every interval of real numbers (bounded or unbounded), the probability that $X$ takes a value in the interval is the integral of $f$ over the interval.\n",
    "\n",
    "**Definition -- Probability Density Function:** If $X$ has a continuous distribution, the function $f$ is called the **probability density function** (abbreviated p.d.f.) of $X$. The closure of the set $\\{x: \\ f(x)>0 \\}$ is called the support of (the distribution of) $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Continuous Distribution\n",
    "\n",
    "Example notation:\n",
    "\n",
    "$$P(a \\leq X \\leq b) = \\int_a^bf(x)dx$$\n",
    "\n",
    "$$P(X \\geq a) = \\int_a^\\infty f(x)dx$$\n",
    "\n",
    "$$P(X \\leq b) = \\int_{-\\infty}^b f(x)dx$$\n",
    "\n",
    "$$P(X = b) = \\int_{b}^b f(x)dx = 0$$\n",
    "\n",
    "![img](https://github.com/umbertomig/CSSBootCamp/blob/main/img/im1.png?raw=true)\n",
    "\n",
    "To think: What is the chance that a person is 6' tall?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Continuous Distribution\n",
    "\n",
    "Example:\n",
    "\n",
    "1. Suppose that the p.d.f. of a random variable X is given by $f(x)$.\n",
    "   - Find the value of the constant $c$ and sketch the p.d.f.\n",
    "   - Find the value of $P(X > 3/2)$.\n",
    "\n",
    "$$f(x) =\n",
    "  \\begin{cases}\n",
    "    cx^2 & \\text{for $1 \\leq x \\leq 2$}, \\\\\n",
    "    0 & \\text{otherwise}\n",
    "  \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Continuous Distribution\n",
    "\n",
    "**Definition -- Uniform Distribution:** Let $a$ and $b$ be two given real numbers such that $a < b$. Let $X$ be $a$ random variable such that it is known that $a \\leq X \\leq b$ and, for every subinterval of $[a, b]$, the probability that $X$ will belong to that subinterval is proportional to the length of that subinterval. We then say that the random variable $X$ has the **uniform distribution on the interval $[a, b]$**.\n",
    "\n",
    "p.d.f: \n",
    "\n",
    "$$f(x) =\n",
    "  \\begin{cases}\n",
    "    \\dfrac{1}{b-a} & \\text{for $a \\leq x \\leq B$}, \\\\\n",
    "    0 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "![img](https://github.com/umbertomig/CSSBootCamp/blob/main/img/im2.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Continuous Distribution\n",
    "\n",
    "**Check-in**\n",
    "\n",
    "1. Suppose that a random variable $X$ has the uniform distribution on the interval $[−2, 8]$. Find the p.d.f. of $X$ and the value of $P(0 < X < 7)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cumulative Distribution Function\n",
    "\n",
    "**Definition -- Cumulative Distribution Function**: The distribution function or cumulative distribution function (abbreviated c.d.f.) $F$ of a random variable $X$ is the function:\n",
    "\n",
    "$$ F(x) = P(X \\leq x), \\quad \\text{ for } -\\infty \\leq x \\leq \\infty$$\n",
    "\n",
    "Note: Any distribution, regardless of the type, has a c.d.f.\n",
    "\n",
    "![img](https://github.com/umbertomig/CSSBootCamp/blob/main/img/im3.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cumulative Distribution Function\n",
    "\n",
    "**Properties:**\n",
    "\n",
    "1. $F$ is non-decreasing as $x$ increases. That is: if $x_1 < x_2$, then $F(x_1) \\leq F(x_2)$.\n",
    "\n",
    "1. $\\lim_{x \\rightarrow -\\infty} F(x) = 0$ and $\\lim_{x \\rightarrow \\infty} F(x) = 1$\n",
    "\n",
    "1. $P(X > x) = 1 - F(x)$\n",
    "\n",
    "1. $P(X > x) = 1 - F(x)$\n",
    "\n",
    "1. $P(x_1 < X < x_2) = F(x_2) - F(x_1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cumulative Distribution Function\n",
    "\n",
    "Example:\n",
    "\n",
    "1. Suppose, that a random variable $X$ has the uniform distribution on the interval [−2, 8]. Find and sketch the c.d.f. of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cumulative Distribution Function\n",
    "\n",
    "**Check-in**\n",
    "\n",
    "1. Suppose that a random variable $X$ can take only the values $−2$, $0$, $1$, and $4$, and that the probabilities of these values are as follows: $P(X = −2) = 0.4$, $P(X = 0) = 0.1$, $P(X = 1) = 0.3$, and $P(X = 4) = 0.2$. Sketch the c.d.f. of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quantile Function\n",
    "\n",
    "*Fair Bets.* Suppose that $X$ is the amount of rain that will fall tomorrow, and $X$ has c.d.f. $F$. Suppose that we want to place an even-money bet on $X$ as follows: If $X \\leq x_0$, we win one dollar and if $X>x_0$ we lose one dollar. \n",
    "\n",
    "*In order to make this bet fair, we need $P(X \\leq x_0) = P(X > x_0) = 1/2$.*\n",
    "\n",
    "We could search through all of the real numbers $x$ trying to find one such that $F(x) = 1/2$, and then we would let $x_0$ equal the value we found. \n",
    "\n",
    "If $F$ is a one-to-one function, then $F$ has an inverse $F^−1$ and $x_0 = F^{−1}(1/2)$.\n",
    "\n",
    "**Definition -- Quantiles/Percentiles:** Let $X$ be a random variable with c.d.f. $F$. For each $p$ strictly between $0$ and $1$, define $F^{−1}(p)$ to be the smallest value $x$ such that $F(x) \\geq p$. Then $F^{−1}(p)$ is called the $p$ quantile of $X$ or the $100p$ percentile of $X$. The function $F^{−1}$ defined here on the open interval $(0, 1)$ is called the quantile function of $X$.\n",
    "\n",
    "![img](https://github.com/umbertomig/CSSBootCamp/blob/main/img/im4.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quantile Functions\n",
    "\n",
    "**Fact -- Quantiles of Continuous Distributions:** When the c.d.f. of a random variable $X$ is continuous and one-to-one over the whole set of possible values of $X$, the inverse $F^{−1}$ of $F$ exists and equals the quantile function of $X$.\n",
    "\n",
    "Example:\n",
    "\n",
    "1. Compute the quantile function of the uniform distribution on the interval $[a, b]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quantile Functions\n",
    "\n",
    "**Fact -- Quantiles of Discrete Distributions**: It is convenient to be able to calculate quantiles for discrete distributions as well.\n",
    "\n",
    "*Quantiles of a Binomial Distribution:* Let $X$ have the binomial distribution with parameters $n = 5$ and $p = 0.3$. The binomial table in the back of the book has the p.f. $f$ of $X$, which we reproduce here together with the $c.d.f.$ F:\n",
    "\n",
    "![img](https://github.com/umbertomig/CSSBootCamp/blob/main/img/im5.png?raw=true)\n",
    "\n",
    "And the quantile function is:\n",
    "\n",
    "![img](https://github.com/umbertomig/CSSBootCamp/blob/main/img/im6.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quantile Functions\n",
    "\n",
    "**Definition -- Median/Quartiles**: The $1/2$ quantile or the 50-th percentile of a distribution is called its median. The $1/4$ quantile or 25-th percentile is the lower quartile. The $3/4$ quantile or 75-th percentile is called the upper quartile.\n",
    "\n",
    "Example: Suppose that $X$ has the p.d.f.\n",
    "\n",
    "$$f(x) =\n",
    "  \\begin{cases}\n",
    "    2x & \\text{for $0 \\leq x \\leq 1$}, \\\\\n",
    "    0 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "1. Find and sketch the c.d.f. or X.\n",
    "1. Find the quantile function and compute:\n",
    "    - The median\n",
    "    - 90-th percentile\n",
    "    - The quartiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T15:45:26.225197Z",
     "iopub.status.busy": "2024-07-24T15:45:26.225109Z",
     "iopub.status.idle": "2024-07-24T15:45:26.227630Z",
     "shell.execute_reply": "2024-07-24T15:45:26.227020Z",
     "shell.execute_reply.started": "2024-07-24T15:45:26.225188Z"
    }
   },
   "outputs": [],
   "source": [
    "## CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bivariate Distributions\n",
    "\n",
    "**Definition -- Joint/Bivariate Distribution**: Let $X$ and $Y$ be random variables. The joint distribution or bivariate distribution of $X$ and $Y$ is the collection of all probabilities of the form $P[(X,Y) \\in C]$ for all sets $C$ of pairs of real numbers such that $\\{(X, Y) \\in C\\}$ is an event.\n",
    "\n",
    "**Definition -- Discrete Joint Distribution**: Let $X$ and $Y$ be random variables, and consider the ordered pair $(X, Y)$. If there are only finitely or at most countably many different possible values $(x, y)$ for the pair $(X, Y)$, then we say that $X$ and $Y$ have a discrete joint distribution.\n",
    "\n",
    "**Definition -- Joint Probability Function, p.f.**: The joint probability function, or the joint p.f., of $X$ and\n",
    "$Y$ is defined as the function $f$ such that for every point $(x, y)$ in the xy-plane:\n",
    "\n",
    "$$f(x, y) = P(X = x \\text{ and } Y = y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bivariate Distributions\n",
    "\n",
    "Example: What is the joint distribution of the clinical trial results below\n",
    "\n",
    "![img](https://github.com/umbertomig/CSSBootCamp/blob/main/img/im13.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bivariate Distributions\n",
    "\n",
    "**Properties**\n",
    "\n",
    "Let X and Y have a discrete joint distribution. \n",
    "\n",
    "1. If $(x, y)$ is ***not*** one of the possible values of the pair $(X, Y)$, then $f(x, y) = 0$.\n",
    "\n",
    "1. $\\sum_{\\text{All } (x,y)} f(x,y) = 1$\n",
    "\n",
    "1. $P[(X,Y) \\in C] = \\sum_{(x,y) \\in C}f(x,y)$\n",
    "\n",
    "Example: In a certain suburban area, each household reported the number of cars and the number of television sets that they owned. Let X stand for the number of cars owned by a randomly selected household in this area. Let Y stand for the number of television sets owned by that same randomly selected household. In this case, X takes only the values 1, 2, and 3; Y takes only the values 1, 2, 3, and 4; and the joint p.f. f of X and Y is:\n",
    "\n",
    "\n",
    "Joint pdf | Plot\n",
    "- | - \n",
    "![alt](https://github.com/umbertomig/CSSBootCamp/blob/main/img/im7.png?raw=true) | ![alt](https://github.com/umbertomig/CSSBootCamp/blob/main/img/im8.png?raw=true)\n",
    "\n",
    "\n",
    "How much is $P(X = 1)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bivariate Distributions\n",
    "\n",
    "**Definition -- Continuous Joint Distribution/Joint p.d.f./Support**: Two random variables $X$ and $Y$ have a continuous joint distribution if there exists a nonnegative function $f$ defined over the entire xy-plane such that for every subset C of the plane,\n",
    "\n",
    "$$P[(X, Y) \\in C] = \\int_C\\int f(x, y)dxdy$$\n",
    " \n",
    "If the integral exists. The function $f$ is called the ***joint probability density function*** (abbreviated joint p.d.f.) of $X$ and $Y$. The closure of the set $\\{(x,y): \\ f(x, y) > 0\\}$ is called the ***support*** of (the distribution of) $(X, Y)$.\n",
    "\n",
    "![alt](https://github.com/umbertomig/CSSBootCamp/blob/main/img/im9.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bivariate Distributions\n",
    "\n",
    "Example -- Calculating a Normalizing Constant:\n",
    "\n",
    "Suppose that the joint p.d.f. of $X$ and $Y$ is specified\n",
    "as follows:\n",
    "\n",
    "$$f(x) =\n",
    "  \\begin{cases}\n",
    "    cx^2y & \\text{for $x^2 \\leq y \\leq 1$}, \\\\\n",
    "    0 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "(easy...ish) What is the value of $c$?\n",
    "(hard) What is $P(X \\geq Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bivariate Distributions\n",
    "\n",
    "(hard) What is $P(X \\geq Y)$\n",
    "\n",
    "Slice 1 | Slice 2\n",
    "- | - \n",
    "![alt](https://github.com/umbertomig/CSSBootCamp/blob/main/img/im10.png?raw=true) | ![alt](https://github.com/umbertomig/CSSBootCamp/blob/main/img/im11.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bivariate Distributions\n",
    "\n",
    "**Definition -- Mixed Bivariate Distributions:** Let $X$ and $Y$ be random variables such that $X$ is discrete and $Y$ is continuous. Suppose that there is a function $f(x, y)$ defined on the xy-plane such that, for every pair $A$ and $B$ of subsets of the real numbers,\n",
    "\n",
    "$$P(X \\in A \\text{ and } Y \\in B) = \\int_B\\sum_{x \\in A}f(x,y)dy$$\n",
    "\n",
    "If the integral exists. Then the function $f$ is called the joint p.f./p.d.f. of $X$ and $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bivariate Distributions\n",
    "\n",
    "**Definition -- Joint (Cumulative) Distribution Function/c.d.f.**: The joint distribution function or joint cumulative distribution function (joint c.d.f.) of two random variables $X$ and $Y$ is defined as the function $F$ such that for all values of $x$ and $y$ ($-\\infty<x<\\infty$ and $-\\infty<y<\\infty$),\n",
    "\n",
    "$$F(x, y) = P(X \\leq x \\text{ and } Y \\leq y)$$\n",
    "\n",
    "Interesting: Assuming $a < b$ and $c < d$, compute\n",
    "\n",
    "$$P(a < X \\leq b \\text{ and } c < Y \\leq d)$$\n",
    "\n",
    "**Fact:** $$f(x,y) = \\dfrac{\\partial^2F(x,y)}{\\partial x \\partial y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bivariate Distributions\n",
    "\n",
    "**Check-in**: \n",
    "\n",
    "Suppose that in an electric display sign there are three light bulbs in the first row and four light bulbs in the second row. Let $X$ denote the number of bulbs in the first row that will be burned out at a specified time $t$, and let $Y$ denote the number of bulbs in the second row that will be burned out at the same time $t$. Suppose that the joint p.f. of $X$ and $Y$ is as specified in the following table:\n",
    "\n",
    "![alt](https://github.com/umbertomig/CSSBootCamp/blob/main/img/im12.png?raw=true)\n",
    "\n",
    "Compute:\n",
    "\n",
    "1. $P(X = 2)$\n",
    "1. $P(Y \\geq 2)$\n",
    "1. $P(X \\leq 2 \\text{ and } Y \\leq 2)$\n",
    "1. $P(X = Y)$\n",
    "1. $P(X > Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Marginal Distributions\n",
    "\n",
    "**Definition -- Marginal c.d.f./p.f./p.d.f.**: Suppose that $X$ and $Y$ have a joint distribution. The c.d.f. of\n",
    "$X$ is called the marginal c.d.f. of $X$. Similarly, the p.f. or p.d.f. of $X$ associated with the marginal c.d.f. of $X$ is called the marginal p.f. or marginal p.d.f. of $X$. \n",
    "\n",
    "**Definition -- Discrete Marginal c.d.f./p.f./p.d.f.**: If $X$ and $Y$ have a discrete joint distribution for which the joint p.f. is $f$, then the marginal p.f. $f_1$ of $X$ is\n",
    "\n",
    "$$f_1(x) = \\sum_{\\text{All }y}f(x, y)$$\n",
    "\n",
    "$f_2(y)$ is defined analogously.\n",
    "\n",
    "For the neighborhood example:\n",
    "\n",
    "![alt](https://github.com/umbertomig/CSSBootCamp/blob/main/img/im14.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Marginal Distributions\n",
    "\n",
    "**Definition -- Continuous Marginal c.d.f./p.f./p.d.f.**: If $X$ and $Y$ have a continuous joint distribution with joint p.d.f. $f$, then the marginal p.d.f. $f_1$ of $X$ is\n",
    "\n",
    "$$f_1(x) = \\int_{-\\infty}^{\\infty}f(x,y)dy$$\n",
    "\n",
    "$f_2(y)$ is defined analogously.\n",
    "\n",
    "Pdf Slice | Marginal\n",
    "- | - \n",
    "![alt](https://github.com/umbertomig/CSSBootCamp/blob/main/img/im15.png?raw=true) | ![alt](https://github.com/umbertomig/CSSBootCamp/blob/main/img/im16.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Independent Random Variables\n",
    "\n",
    "**Definition -- Independent Random Variables**: Two random variables X and Y are independent if, for every two sets $A$ and $B$ of real numbers such that $\\{X \\in A\\}$ and\n",
    "$\\{Y \\in B\\}$ are events,\n",
    "\n",
    "$$P(X \\in A \\text{ and } Y \\in B) = P(X \\in A) P(Y \\in B)$$\n",
    "\n",
    "**Fact**: Two random variables $X$ and $Y$ are independent if and only if the following factorization is satisfied for all real numbers x and y:\n",
    "\n",
    "$$f(x, y) = f_1(x)f_2(y)$$\n",
    "\n",
    "**Interpretation**: \n",
    "\n",
    "- For events: Learning that one of them occurs does not change the probability that the other one occurs.\n",
    "\n",
    "- For r.v.s (random variables): Learning a realization of one does not change the chances of the other.\n",
    "\n",
    "![alt](https://github.com/umbertomig/CSSBootCamp/blob/main/img/im17.png?raw=true) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Independent Random Variables\n",
    "\n",
    "Example:\n",
    "\n",
    "Suppose that the joint p.d.f. of $X$ and $Y$ is as follows:\n",
    "\n",
    "$$f(x, y) =\n",
    "  \\begin{cases}\n",
    "    \\dfrac{15}{4}x^2 & \\text{for $0 \\leq y \\leq 1-x^2$}, \\\\\n",
    "    0 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "1. Determine the marginal p.d.f.’s of $X$ and $Y$.\n",
    "1. Are $X$ and $Y$ independent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Marginal Distributions\n",
    "\n",
    "**Check-in**: Suppose that $X$ and $Y$ have a continuous joint distribution for which the joint p.d.f. is\n",
    "\n",
    "$$f(x, y) =\n",
    "  \\begin{cases}\n",
    "    k & \\text{for $a \\leq x \\leq b$ and $c \\leq y \\leq d$}, \\\\\n",
    "    0 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "Find the marginal distributions of $X$ and $Y$. Are $X$ and $Y$ independent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Distributions\n",
    "\n",
    "**Definition -- Conditional Distribution/p.f.**: Let $X$ and $Y$ have a discrete joint distribution with joint\n",
    "p.f. $f$. Let $f_2$ denote the marginal p.f. of $Y$. For each $y$ such that $f_2(y) > 0$, define:\n",
    "\n",
    "$$g_1(x|y) = \\dfrac{f(x, y)}{f2(y)}$$\n",
    "\n",
    "Then $g_1$ is called the **conditional p.f. of X given Y**. The discrete distribution whose p.f. is $g_1(.|y)$ is called the **conditional distribution of X given that $Y = y$**.\n",
    "\n",
    "Example: Auto insurance\n",
    "\n",
    "Pdf Slice | Marginal\n",
    "- | - \n",
    "![alt](https://github.com/umbertomig/CSSBootCamp/blob/main/img/im18.png?raw=true) | ![alt](https://github.com/umbertomig/CSSBootCamp/blob/main/img/im19.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Distributions\n",
    "\n",
    "![alt](https://github.com/umbertomig/CSSBootCamp/blob/main/img/im20.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Independence\n",
    "\n",
    "**Theorem -- Implications of Independence**: Let $X$ and $Y$ two random variables with joint *p.f.* $f$. The following statements are equivalent:\n",
    "\n",
    "1. $X \\perp Y$\n",
    "2. $\\forall x,y \\in \\mathbb{R}$, $f(x, y) = f_{X}(x)f_{Y}(y)$\n",
    "3. $\\forall x \\in \\mathbb{R}$ and $y \\in \\text{Supp}[Y]$, $f_{X|Y}(x|y) = f_{X}(x)$\n",
    "4. $\\forall D, E \\subseteq \\mathbb{R}$, the events $\\{X \\in D\\}$ and $\\{Y \\in E\\}$ are independent.\n",
    "5. For all functions $g$ of $X$ and $h$ of $Y$ $g(X) \\perp h(Y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multivariate Generalizations\n",
    "\n",
    "- (Definition) **Random Vectors**: A random vector is a function $X: S \\rightarrow \\mathbb{R}^{K}$ such that, for all event A, $\\bX(A) \\ = \\ \\bigg( X_{1}(A), \\cdots, X_{K}(A) \\bigg)$. And where $X_{i}$ is a random variable.\n",
    "\n",
    "- (Definition) **Joint Cumulative Density Function**: For a random vector $\\bX$, evaluated at $\\bvx$, is denoted $F(\\bvx) = \\mathbb{P}\\left[ \\bX \\leq \\bvx \\right]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multivariate Generalizations\n",
    "\n",
    "And from here, the extensions are evident:\n",
    "\n",
    "- For continuous random vectors, we use multiple integrals (*CDF*s) or partial derivatives (*PDF*s)\n",
    "\n",
    "- For discrete random vectors, we use multiple sums (*CDF*s).\n",
    "\n",
    "- For the conditional *PDFs/PMFs*, we marginalize on the variables by summing up across their support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing Distributions\n",
    "\n",
    "- Statistics would not be very useful if we did not find ways to summarize the objects we work with.\n",
    "\n",
    "- **Definition -- Expected Value**: For a random variable $X$ with bounded variation:\n",
    "  1. If $X$ is discrete, then $\\Ex[X] \\ = \\ \\sum_{x}xf(x)$\n",
    "  + If $X$ is continuous, then $\\Ex[X] \\ = \\ \\int_{-\\infty}^{\\infty}xf(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing Distributions\n",
    "\n",
    "- (Theorem) **Expectation of a Function**: For a random variable $X$ with *PMF/PDF* $f$ and a function $g$ (still assuming bounded variation):\n",
    "  1. If $X$ is discrete, then $\\Ex [g(X)] \\ = \\ \\sum_{x}g(x)f(x)$\n",
    "  + If $X$ is continuous, then $\\Ex [g(X)] \\ = \\ \\int_{-\\infty}^{\\infty}g(x)f(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing Distributions\n",
    "\n",
    "- (Theorem) **Linearity of Expected Values**: For a random variable $X$, and $a, b \\in \\mathbb{R}$, then:\n",
    "\n",
    "$$\\Ex[aX + b] \\ = \\ a\\Ex[X] + b$$\n",
    "\n",
    "- (Definition) **Expectation of a Bivariate Random Vector**: For a random vector $(X,Y)$, the expected value is $\\Ex [(X, Y)] \\ = \\ \\bigg(\\Ex[X], \\Ex[Y] \\bigg)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing Distributions\n",
    "\n",
    "- (Definition) **Raw Moment**: For a random variable $X$, the $j$-th *raw moment* is defined as:\n",
    "\n",
    "$$ \\mu_{j}' = \\Ex [X^{j}] $$\n",
    "\n",
    "- (Definition) **Central Moment**: For a random variable $X$, the $j$-th *central moment* is defined as:\n",
    "\n",
    "$$ \\mu_{j} = \\Ex \\left[(X -\\Ex [X])^{j} \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing Distributions\n",
    "\n",
    "- (Definition) **Variance**: For a random variable $X$, the variance is defined as the *second central moment* of $X$:\n",
    "\n",
    "$$ \\Vax [X] = \\Ex \\left[(X -\\Ex [X])^{2} \\right] $$\n",
    "\n",
    "- (Definition) **Standard Deviation**: For a random variable $X$, the variance is defined as $\\sigma [X] = \\sqrt{\\Vax[X]}$\n",
    "\n",
    "- (Theorem) **Alternative formula for the Variance**: For a random variable $X$, the variance is defined as the *second central moment* of $X$:\n",
    "\n",
    "$$ \\Vax [X] = \\Ex [X^2] - \\left[ \\Ex [X] \\right]^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing Distributions\n",
    "\n",
    "- (Theorem) **Algebra of the Variance**: For a random variable $X$ and $a \\in \\mathbb{R}$, $b \\in \\mathbb{R}$:\n",
    "\n",
    "$$ \\Vax [aX + b] \\ = \\ a^{2} \\Vax [X] $$\n",
    "\n",
    "- (Corollary) **Algebra of the Standard Deviations**: Let a random variable $X$, $a \\in \\mathbb{R}$, and $b \\in \\mathbb{R}$. Then, $\\sigma [aX + b] = |a| \\sigma [X]$.\n",
    "\n",
    "- (Theorem) **Chebyshev Inequality**: Let a random variable $X$ and $\\sigma [X] > 0$. Then, $\\forall \\epsilon > 0$:\n",
    "\n",
    "$$ \\mathbb{P} \\bigg[ \\big| X - \\Ex[X] \\big| \\geq \\epsilon \\sigma [X] \\bigg] \\leq \\dfrac{1}{\\epsilon^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing Distributions\n",
    "\n",
    "- (Definition) **Normal Distribution**: A continuous random variable $X$ follows a normal distribution (denoted by $X \\sim N(\\mu, \\sigma^2)$) if:\n",
    "\n",
    "$$ f(x) \\ = \\ \\dfrac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\dfrac{(x - \\mu)^{2}}{2\\sigma^2}} $$\n",
    "\n",
    "- (Theorem) **Mean and Standard Deviation of a Normal Distribution**: Let $X \\sim N(\\mu, \\sigma^2)$. Then, $\\Ex[X] \\ = \\ \\mu$ and $\\sigma[X] \\ = \\ \\sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing Distributions\n",
    "\n",
    "- (Theorems) **Algebra of Normal Distributions**: Let $X \\sim N(\\mu_{X}, \\sigma_{X}^2)$, $Y \\sim N(\\mu_{Y}, \\sigma_{Y}^2)$, and $a, b \\in \\mathbb{R}$, $a \\neq 0$. Then:\n",
    "  1. If $W = aX + b$, then $W \\sim N(a\\mu_{X} + b, a^2\\sigma_{X}^2)$\n",
    "  2. If $X \\ind Y$, and $Z = X + Y$, then $Z \\sim N(\\mu_{X} + \\mu_{Y}, \\sigma_{X}^2 + \\sigma_{Y}^2)$\n",
    "\n",
    "- This proof should be straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing Distributions\n",
    "\n",
    "- And one of the most important objects: suppose we want to say how well a random variable $X$ approximates a given value $c \\in \\mathbb{R}$.\n",
    "\n",
    "- The most common metric we use for this purpose is the *Mean Squared Error*.\n",
    "\n",
    "- (Definition) **Mean Squared Error**: The *MSE* of a random variable $X$ about $c$ is equal to: $\\Ex[(X - c)^2]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing Distributions\n",
    "\n",
    "- (Theorem) **Alternative formulation for the Mean Squared Error**: The *MSE* of a random variable about $c$ is equal to:\n",
    "\n",
    "$$ \\Ex[(X - c)^2] \\ = \\ \\Vax[X] + (\\Ex[X] - c)^2 $$\n",
    "\n",
    "- (Theorem) **Mean Squared Error Minimization**: The value $c$ that minimizes the *MSE* of a random variable $X$ about $c$ is $c = \\Ex[X]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing Distributions (Joint Distributions)\n",
    "\n",
    "- (Definition) **Covariance**: The covariance of two random variables is defined as \n",
    "$$\\text{Cov}[X, Y] = \\big[ \\big(X - \\Ex[X]\\big)\\big(Y - \\Ex[Y]\\big) \\big] = \\Ex[XY] - \\Ex[X]\\Ex[Y]$$\n",
    "\n",
    "- (Theorem) **Variance Rule**: Let $X$ and $Y$ two r.v. Then,\n",
    "\n",
    "1. $$\\Vax[X + Y] = \\Vax[X] +2\\text{Cov}[X, Y] + \\Vax[Y]$$\n",
    "\n",
    "2. And if $a, b, c \\in \\real$, then $$\\Vax[aX + bY + c] = a^2\\Vax[X] +2ab\\text{Cov}[X, Y] + b^2\\Vax[Y]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing Distributions (Joint Distributions)\n",
    "\n",
    "- (Theorem) **Algebra of Covariance**: Let $X$, $Y$, $Z$, $W$ four r.v.s and $a,b,c,d \\in \\real$. Then:\n",
    "\n",
    "1. $\\text{Cov}[X, c]=\\text{Cov}[c, X]=\\text{Cov}[d, c]$\n",
    "2. $\\text{Cov}[X, Y] = \\text{Cov}[Y, X]$\n",
    "3. $\\text{Cov}[X, X] = \\Vax[X]$\n",
    "4. \n",
    "$$\\text{Cov}[X + W, Y + Z]=\\text{Cov}[X, Z]+\\text{Cov}[X, Y]+\\text{Cov}[W, Y]+\\text{Cov}[W,Z]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing Distributions (Joint Distributions)\n",
    "\n",
    "(Definition) **Correlation**: The correlation of two random variables $X$ and $Y$, with $\\sigma[X]>0$ and $\\sigma[Y]>0$ is defined as \n",
    "$$\\rho[X, Y] = \\dfrac{\\cov[X,Y]}{\\sigma[X]\\sigma[Y]}$$\n",
    "\n",
    "(Theorem) **Correlation and Linear Dependence**: Let two random variables $X$ and $Y$, with $\\sigma[X]>0$ and $\\sigma[Y]>0$. Then:\n",
    "\n",
    "1. $\\rho[X, Y] \\in [-1, 1]$\n",
    "\n",
    "2. For $a, b \\in \\real$, and $Y = aX + b$:\n",
    "    - $\\rho[X, Y] = 1$ if $a > 0$\n",
    "    - $\\rho[X, Y] = -1$ if $a < 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summarizing Distributions (Joint Distributions)\n",
    "\n",
    "(Theorem) **Properties of Correlation**: Let random variables $X$, $Y$, and $Z$, all with variance higher than zero. Let also $a,b,c,d \\in \\real$.\n",
    "1. $\\rho[X, Y] = \\rho[Y, X]$\n",
    "2. $\\rho[X, X] = 1$\n",
    "3. If $ab>0$, then $\\rho[aX + c, bY + d] = \\rho[X,Y]$\n",
    "4. If $ab<0$, then $\\rho[aX + c, bY + d] = -\\rho[X,Y]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Independence\n",
    "\n",
    "(Theorem) **Independence**: Let $X$ and $Y$ two independents r.v.s. Then:\n",
    "1. $\\rho[X, Y] = 0$\n",
    "2. $\\cov[X, Y] = 0$\n",
    "3. $\\Ex[XY] = \\Ex[X]\\Ex[Y]$\n",
    "4. $\\Vax[X + Y] = \\Vax[X] + \\Vax[Y]$\n",
    "\n",
    "- However, these statements are not equivalent, since you may have $\\rho[X, Y] = 0$ without having independence!\n",
    "  + See pages 65 and 66 of the Aronow and Miller book (especially the footnote on pg 66 for a notable exception)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Expectation Functions\n",
    "\n",
    "(Definition) **Conditional Expectation**: For two random variables $X$ and $Y$, the conditional expectation of $Y$ given $X=x$ is:\n",
    "\n",
    "1. If $X$ and $Y$ are discrete, and $x \\in \\text{Supp}[X]$, then $$\\Ex[Y|X=x] = \\sum_{y}yf_{Y|X}(y|x)$$\n",
    "\n",
    "2. If $X$ and $Y$ are continuous, and $x \\in \\text{Supp}[X]$, then $$\\Ex[Y|X=x] = \\int_{-\\infty}^{\\infty}yf_{Y|X}(y|x)dy$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Expectation Functions\n",
    "\n",
    "(Definition) **Conditional Expectation of a function**: For two random variables $X$ and $Y$ and a function of $X$ and $Y$, the conditional expectation of $h(X,Y)$ given $X=x$ is:\n",
    "\n",
    "1. If $X$ and $Y$ are discrete, and $x \\in \\text{Supp}[X]$, then $$\\Ex[h(X,Y)|X=x] = \\sum_{y}h(x,y)f_{Y|X}(y|x)$$\n",
    "\n",
    "2. If $X$ and $Y$ are continuous, and $x \\in \\text{Supp}[X]$, then $$\\Ex[h(X,Y)|X=x] = \\int_{-\\infty}^{\\infty}h(x,y)f_{Y|X}(y|x)dy$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Expectation Functions\n",
    "\n",
    "(Theorem) **Linearity of Conditional Expectation**: Let $X$ and $Y$ rvs. If $g$ and $h$ are functions (with $x \\in \\text{Supp}[X]$ is: \n",
    "\n",
    "$$\\Ex[g(X)Y + h(X) | X=x] = g(x)\\Ex[Y | X=x] + h(x)$$\n",
    "\n",
    "(Definition) **Conditional Expectation Function**: Let $X$ and $Y$ rvs with joint distribution $f$ ($x \\in \\text{Supp}[X]$) is: \n",
    "\n",
    "$$G_Y(x) = \\Ex[Y | X=x]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Expectation Functions\n",
    "\n",
    "(Theorem) **Law of Iterated Expectations**: Let $X$ and $Y$ rvs. \n",
    "\n",
    "$$\\Ex[Y] = \\Ex\\big[\\Ex[Y | X]\\big]$$\n",
    "\n",
    "(Theorem) **Law of Total Variance**: Let $X$ and $Y$ rvs. \n",
    "\n",
    "$$\\Vax[Y] = \\Ex\\big[\\Vax[Y | X]\\big] + \\Vax\\big[\\Ex[Y | X]\\big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Expectation Functions\n",
    "\n",
    "(Theorem) **Properties of Deviations from the CEF**: Let $X$ and $Y$ rvs and let $\\epsilon = Y - \\Ex[Y|X]$.\n",
    "\n",
    "1. $\\Ex[\\epsilon|X] = 0$\n",
    "2. $\\Ex[\\epsilon] = 0$\n",
    "3. If $g$ is a function of $X$, $\\cov\\big[g(X),\\epsilon \\big] = 0$\n",
    "4. $\\Vax[\\epsilon|X] = \\Vax[Y|X]$\n",
    "5. $\\Vax[\\epsilon] = \\Ex\\big[\\Vax[Y|X]\\big]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Expectation Functions\n",
    "\n",
    "- (Theorem) **Properties of Deviations from the CEF**: Let $X$ and $Y$ rvs and let $\\epsilon = Y - \\Ex[Y|X]$.\n",
    "\n",
    "1. $\\Ex[\\epsilon|X] = 0$\n",
    "2. $\\Ex[\\epsilon] = 0$\n",
    "3. If $g$ is a function of $X$, $\\cov\\big[g(X),\\epsilon \\big] = 0$\n",
    "4. $\\Vax[\\epsilon|X] = \\Vax[Y|X]$\n",
    "5. $\\Vax[\\epsilon] = \\Ex\\big[\\Vax[Y|X]\\big]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Expectation Function\n",
    "\n",
    "(Theorem) **CEF and Best Linear Predictor**: Let $X$ and $Y$ rvs, $\\Ex[Y|X]$ is the best predictor of $Y$ given $X$.\n",
    "\n",
    "(Theorem) **Best Linear Predictor**: Let $X$ and $Y$ rvs, if $\\Vax[X] > 0$, then the BLP of $Y$ given $X$ is $g(X) = \\beta_0 + \\beta_1X$ where:\n",
    "\n",
    "1. $\\beta_0 = \\Ex[Y] - \\beta_1\\Ex[X]$\n",
    "\n",
    "2. $\\beta_1 = \\dfrac{\\cov[X,Y]}{\\Vax[X]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Expectation Function\n",
    "\n",
    "(Theorem) **Implications of Independence**: Let $X$ and $Y$ independent rvs.:\n",
    "\n",
    "1. $\\Ex[Y|X] = \\Ex[Y]$\n",
    "2. $\\Vax[Y|X] = \\Vax[Y]$\n",
    "3. The BLP of $Y$ given $X$ is $\\Ex[Y]$\n",
    "4. If $g$ is a function of $X$ and $h$ is a function of $Y$:\n",
    "    - $\\Ex[h(Y)|g(X)] = \\Ex[h(Y)]$\n",
    "    - The BLP of $h(Y)$ given $g(X)$ is $\\Ex[h(X)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Great work!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
